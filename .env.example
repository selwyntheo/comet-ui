# GraphQL API Configuration
VITE_GRAPHQL_ENDPOINT=http://localhost:4000/graphql

# Llama LLM Configuration
# For Ollama (local Llama deployment) - Default recommended setup
VITE_LLAMA_API_URL=http://localhost:11434/v1
VITE_LLAMA_MODEL=llama3.1:8b
# API key not required for local Ollama, but may be needed for hosted services
VITE_LLAMA_API_KEY=

# Alternative Llama Configurations:

# For LLaMA.cpp server
# VITE_LLAMA_API_URL=http://localhost:8080/v1
# VITE_LLAMA_MODEL=llama-2-7b-chat

# For Hugging Face Inference API (Llama models)
# VITE_LLAMA_API_URL=https://api-inference.huggingface.co/models
# VITE_LLAMA_MODEL=meta-llama/Llama-2-7b-chat-hf
# VITE_LLAMA_API_KEY=your_huggingface_token

# For Together AI (Llama models)
# VITE_LLAMA_API_URL=https://api.together.xyz/v1
# VITE_LLAMA_MODEL=meta-llama/Llama-2-7b-chat-hf
# VITE_LLAMA_API_KEY=your_together_api_key

# For Anyscale (Llama models)
# VITE_LLAMA_API_URL=https://api.endpoints.anyscale.com/v1
# VITE_LLAMA_MODEL=meta-llama/Llama-2-7b-chat-hf
# VITE_LLAMA_API_KEY=your_anyscale_api_key

# Application Configuration
VITE_APP_NAME=CarDoc AI Agent
VITE_APP_VERSION=1.0.0
